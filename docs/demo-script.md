# Demo Script — Akamai Edge AI Market Analyst

**Presenter run-of-show for the live demo.**

---

## Before the Demo (15 min prior)

```bash
# On VM1 (orchestrator):
make test              # Verify GPU, Ollama, VLAN, API, frontend
make warmup            # Pre-load models into VRAM (~5s each)
make status            # Confirm models loaded, GPU memory allocated
```

Check the browser:
- Open `http://<VM1-public-ip>:8000`
- Verify the UI loads (dark theme, input field visible)
- Click the infrastructure panel — confirm both VMs show green

## Fallback: Mock Mode

If anything goes wrong with the GPUs or Ollama during the demo:

```bash
# On VM1, stop the running app and restart in mock mode:
make clean
MOCK_MODE=true make backend &
```

Mock mode streams realistic agent activity with real chart generation in ~30 seconds. The audience won't know the difference.

---

## Demo Flow (~4-5 minutes)

### 1. Set the Scene (30s)

> "We're running two Akamai GPU instances in Seattle — an RTX 6000 Pro as the orchestrator running a 27-billion parameter model, and an RTX 4000 Ada running a 12-billion parameter model for the specialist agents. All model traffic stays on a private network between the two machines."

Point out the infrastructure panel at the bottom of the UI.

### 2. Start the Analysis (15s)

> "Let's ask it to analyze the competitive landscape for edge AI inference providers."

- Select the preset topic from the dropdown, or type a custom one
- Click **Go**

### 3. Watch Agent Activity (2-3 min)

The left panel streams real-time agent activity. Narrate as agents appear:

> **Manager (navy badge):** "The 27B manager model is breaking down our request into subtasks and delegating..."

> **Researcher (blue badge):** "The researcher is gathering market data — key players, trends, competitive dynamics. This is running on the 12B model on the second GPU."

> **Analyst (teal badge):** "Now the analyst is transforming those findings into quantitative datasets for visualization."

> **Visualizer (purple badge):** "The visualizer is calling our chart generation tool — you'll see the charts appear in real-time."

> **Writer (amber badge):** "Finally the writer is producing a polished markdown report with embedded chart references."

### 4. Show the Report (30s)

When the run completes, the report appears on the right panel with inline charts.

> "In about [X] minutes, we got a full market analysis with executive summary, competitive breakdown, data visualizations, and strategic recommendations — all generated by AI agents running on Akamai infrastructure."

### 5. Key Talking Points

Pick 2-3 based on your audience:

- **Cost efficiency:** "Two GPU instances in Seattle. The 6000 Pro for orchestration, the 4000 Ada for the worker agents. This is a fraction of the cost of comparable cloud GPU offerings."

- **Data privacy:** "All model inference happens on our own hardware. No data leaves our infrastructure — the models run locally, and inter-agent communication stays on the private network."

- **Akamai's edge network:** "Imagine deploying inference endpoints at the edge — closer to users, lower latency, with Akamai's network handling the routing."

- **Hierarchical agents:** "The manager model is larger and smarter — it plans and delegates. The specialist models are smaller and cheaper — they execute. This mirrors how real teams work."

---

## Q&A Prep

| Question | Answer |
|----------|--------|
| "What models are you using?" | Gemma 3 by Google — 27B for the manager, 12B for specialists. Open-weight, runs great on consumer/prosumer GPUs via Ollama. |
| "How long does a typical run take?" | 3-5 minutes depending on topic complexity. The mock mode demo runs in ~30 seconds. |
| "Could this run at the edge?" | Absolutely — that's the vision. Smaller models on edge GPUs, orchestrated by a larger model in a regional hub. |
| "What about web search / RAG?" | This demo uses model knowledge only. Adding web search or RAG tools is straightforward with CrewAI's tool system. |
| "What framework is this?" | CrewAI — a Python framework for multi-agent orchestration. Hierarchical process mode lets a manager agent delegate to specialists. |

---

## Post-Demo Cleanup

```bash
make clean      # Stop containers, clear generated output
```
